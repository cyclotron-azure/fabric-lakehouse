{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be00429d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:42:26 - __main__ - INFO - Starting Silver layer transformation\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.logging_utils import setup_logger, log_dataframe_stats\n",
    "from src.quality_checks import check_data_quality\n",
    "from src.silver import clean_customer_data, clean_order_data, join_orders_with_customers, compute_monthly_revenue\n",
    "\n",
    "# Setup logger\n",
    "logger = setup_logger(__name__, level=\"INFO\")\n",
    "logger.info(\"Starting Silver layer transformation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071e79d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/10 12:42:30 WARN Utils: Your hostname, kdcllc-ThinkPad-P16-Gen-2 resolves to a loopback address: 127.0.1.1; using 192.168.86.233 instead (on interface wlp0s20f3)\n",
      "26/02/10 12:42:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/10 12:42:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:42:31 - __main__ - INFO - Created new Spark session\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session (simplified for local development)\n",
    "try:\n",
    "    spark\n",
    "    logger.info(\"Using existing Spark session\")\n",
    "except NameError:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Silver_Transform\") \\\n",
    "        .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "        .getOrCreate()\n",
    "    logger.info(\"Created new Spark session\")\n",
    "\n",
    "# Configuration\n",
    "BRONZE_PATH = \"Tables/bronze\"\n",
    "SILVER_PATH = \"Tables/silver\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dccc238",
   "metadata": {},
   "source": [
    "## Step 1: Load Bronze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6168144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:42:35 - __main__ - INFO - Loading Bronze layer data...\n",
      "2026-02-10 12:42:37 - __main__ - INFO - DataFrame 'customers_bronze' statistics:\n",
      "2026-02-10 12:42:37 - __main__ - INFO -   - Rows: 30\n",
      "2026-02-10 12:42:37 - __main__ - INFO -   - Columns: 9\n",
      "2026-02-10 12:42:37 - __main__ - INFO -   - Column names: customer_id, name, email, phone, signup_date, extra_info, ingestion_timestamp, source_file, bronze_layer_id\n",
      "2026-02-10 12:42:38 - __main__ - INFO -   - Null counts:\n",
      "2026-02-10 12:42:38 - __main__ - INFO -     customer_id: 1 (3.33%)\n",
      "2026-02-10 12:42:38 - __main__ - INFO -     name: 1 (3.33%)\n",
      "2026-02-10 12:42:38 - __main__ - INFO -     email: 1 (3.33%)\n",
      "2026-02-10 12:42:38 - __main__ - INFO -     phone: 1 (3.33%)\n",
      "2026-02-10 12:42:38 - __main__ - INFO -     signup_date: 1 (3.33%)\n",
      "2026-02-10 12:42:38 - __main__ - INFO -     extra_info: 24 (80.00%)\n",
      "2026-02-10 12:42:38 - __main__ - INFO - DataFrame 'orders_bronze' statistics:\n",
      "2026-02-10 12:42:38 - __main__ - INFO -   - Rows: 36\n",
      "2026-02-10 12:42:38 - __main__ - INFO -   - Columns: 10\n",
      "2026-02-10 12:42:38 - __main__ - INFO -   - Column names: order_id, order_date, customer_id, status, quantity, price, notes, ingestion_timestamp, source_file, bronze_layer_id\n",
      "2026-02-10 12:42:38 - __main__ - INFO -   - Null counts:\n",
      "2026-02-10 12:42:38 - __main__ - INFO -     order_date: 1 (2.78%)\n",
      "2026-02-10 12:42:38 - __main__ - INFO -     customer_id: 1 (2.78%)\n",
      "2026-02-10 12:42:38 - __main__ - INFO -     status: 1 (2.78%)\n",
      "2026-02-10 12:42:38 - __main__ - INFO -     quantity: 1 (2.78%)\n",
      "2026-02-10 12:42:38 - __main__ - INFO -     price: 1 (2.78%)\n",
      "2026-02-10 12:42:38 - __main__ - INFO -     notes: 21 (58.33%)\n"
     ]
    }
   ],
   "source": [
    "# Read Bronze tables (Parquet format for local development)\n",
    "logger.info(\"Loading Bronze layer data...\")\n",
    "\n",
    "customers_bronze = spark.read.format(\"parquet\").load(f\"{BRONZE_PATH}/customers\")\n",
    "orders_bronze = spark.read.format(\"parquet\").load(f\"{BRONZE_PATH}/orders\")\n",
    "\n",
    "log_dataframe_stats(customers_bronze, \"customers_bronze\", logger)\n",
    "log_dataframe_stats(orders_bronze, \"orders_bronze\", logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61805b1",
   "metadata": {},
   "source": [
    "## Step 2: Data Quality Checks\n",
    "\n",
    "Run quality checks to identify issues before transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c21cf778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quality issue in customers.customer_id: NULL_VALUES (1 rows)\n",
      "Quality issue in customers.name: NULL_VALUES (1 rows)\n",
      "Quality issue in customers.email: NULL_VALUES (1 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Customer Data Quality Score: 90.00%\n",
      "Total Issues: 3\n",
      "  - NULL_VALUES: customer_id (1 rows)\n",
      "  - NULL_VALUES: name (1 rows)\n",
      "  - NULL_VALUES: email (1 rows)\n"
     ]
    }
   ],
   "source": [
    "# Quality check on customers\n",
    "customer_quality = check_data_quality(\n",
    "    customers_bronze,\n",
    "    \"customers\",\n",
    "    required_columns=[\"customer_id\", \"name\", \"email\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nCustomer Data Quality Score: {customer_quality.quality_score:.2f}%\")\n",
    "print(f\"Total Issues: {len(customer_quality.issues)}\")\n",
    "for issue in customer_quality.issues:\n",
    "    print(f\"  - {issue['type']}: {issue['column']} ({issue['count']} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eee4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality check on orders\n",
    "order_quality = check_data_quality(\n",
    "    orders_bronze,\n",
    "    \"orders\",\n",
    "    required_columns=[\"order_id\", \"customer_id\", \"order_date\", \"quantity\", \"price\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nOrder Data Quality Score: {order_quality.quality_score:.2f}%\")\n",
    "print(f\"Total Issues: {len(order_quality.issues)}\")\n",
    "for issue in order_quality.issues:\n",
    "    print(f\"  - {issue['type']}: {issue['column']} ({issue['count']} rows) - {issue['details']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb61a72",
   "metadata": {},
   "source": [
    "## Step 3: Clean and Transform Data\n",
    "\n",
    "Apply cleaning logic from src/silver.py module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d136789c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:42:49 - __main__ - INFO - DataFrame 'customers_clean' statistics:\n",
      "2026-02-10 12:42:49 - __main__ - INFO -   - Rows: 28\n",
      "2026-02-10 12:42:49 - __main__ - INFO -   - Columns: 11\n",
      "2026-02-10 12:42:49 - __main__ - INFO -   - Column names: customer_id, name, email, phone, signup_date, extra_info, ingestion_timestamp, source_file, bronze_layer_id, has_email, has_phone\n",
      "2026-02-10 12:42:49 - __main__ - INFO -   - Null counts:\n",
      "2026-02-10 12:42:49 - __main__ - INFO -     email: 1 (3.57%)\n",
      "2026-02-10 12:42:49 - __main__ - INFO -     phone: 1 (3.57%)\n",
      "2026-02-10 12:42:49 - __main__ - INFO -     signup_date: 1 (3.57%)\n",
      "2026-02-10 12:42:49 - __main__ - INFO -     extra_info: 24 (85.71%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: string, name: string, email: string, phone: string, signup_date: date, extra_info: string, ingestion_timestamp: timestamp, source_file: string, bronze_layer_id: bigint, has_email: boolean, has_phone: boolean]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean customers data\n",
    "customers_clean = clean_customer_data(customers_bronze)\n",
    "log_dataframe_stats(customers_clean, \"customers_clean\", logger)\n",
    "\n",
    "display(customers_clean.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "893df212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:42:51 - __main__ - INFO - DataFrame 'orders_clean' statistics:\n",
      "2026-02-10 12:42:51 - __main__ - INFO -   - Rows: 0\n",
      "2026-02-10 12:42:51 - __main__ - INFO -   - Columns: 11\n",
      "2026-02-10 12:42:51 - __main__ - INFO -   - Column names: order_id, order_date, customer_id, status, quantity, price, notes, ingestion_timestamp, source_file, bronze_layer_id, line_total\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Clean orders data\u001b[39;00m\n\u001b[32m      2\u001b[39m orders_clean = clean_order_data(orders_bronze)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mlog_dataframe_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43morders_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morders_clean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Note: Status is now normalized to lowercase\u001b[39;00m\n\u001b[32m      6\u001b[39m display(orders_clean.select(\u001b[33m\"\u001b[39m\u001b[33morder_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcustomer_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33morder_date\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquantity\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprice\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mline_total\u001b[39m\u001b[33m\"\u001b[39m).limit(\u001b[32m10\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Internviews/fabric-lakehouse/notebooks/../src/logging_utils.py:70\u001b[39m, in \u001b[36mlog_dataframe_stats\u001b[39m\u001b[34m(df, name, logger)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Log null counts for each column\u001b[39;00m\n\u001b[32m     65\u001b[39m null_counts = df.select([\n\u001b[32m     66\u001b[39m     (F.sum(F.col(c).isNull().cast(\u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m)).alias(c)) \n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df.columns\n\u001b[32m     68\u001b[39m ]).collect()[\u001b[32m0\u001b[39m].asDict()\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m has_nulls = \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnull_counts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_nulls:\n\u001b[32m     72\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33m  - Null counts:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Internviews/fabric-lakehouse/notebooks/../src/logging_utils.py:70\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Log null counts for each column\u001b[39;00m\n\u001b[32m     65\u001b[39m null_counts = df.select([\n\u001b[32m     66\u001b[39m     (F.sum(F.col(c).isNull().cast(\u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m)).alias(c)) \n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df.columns\n\u001b[32m     68\u001b[39m ]).collect()[\u001b[32m0\u001b[39m].asDict()\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m has_nulls = \u001b[38;5;28many\u001b[39m(\u001b[43mcount\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m \u001b[38;5;28;01mfor\u001b[39;00m count \u001b[38;5;129;01min\u001b[39;00m null_counts.values())\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_nulls:\n\u001b[32m     72\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33m  - Null counts:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: '>' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "# Clean orders data\n",
    "orders_clean = clean_order_data(orders_bronze)\n",
    "log_dataframe_stats(orders_clean, \"orders_clean\", logger)\n",
    "\n",
    "# Note: Status is now normalized to lowercase\n",
    "display(orders_clean.select(\"order_id\", \"customer_id\", \"order_date\", \"status\", \"quantity\", \"price\", \"line_total\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef8e99",
   "metadata": {},
   "source": [
    "## Step 4: Enrich Orders with Customer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f0b5053",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `city` cannot be resolved. Did you mean one of the following? [`email`, `name`, `phone`, `has_email`, `customer_id`].;\n'Project [customer_id#0, name#1, email#271, 'city, 'state]\n+- Project [customer_id#0, name#1, email#271, phone#3, signup_date#4, extra_info#5, ingestion_timestamp#6, source_file#7, bronze_layer_id#8L, has_email#281, (isnotnull(phone#3) AND NOT (phone#3 = )) AS has_phone#292]\n   +- Project [customer_id#0, name#1, email#271, phone#3, signup_date#4, extra_info#5, ingestion_timestamp#6, source_file#7, bronze_layer_id#8L, (isnotnull(email#271) AND NOT (email#271 = )) AS has_email#281]\n      +- Deduplicate [customer_id#0]\n         +- Project [customer_id#0, name#1, lower(trim(email#2, None)) AS email#271, phone#3, signup_date#4, extra_info#5, ingestion_timestamp#6, source_file#7, bronze_layer_id#8L]\n            +- Filter (isnotnull(name#1) AND NOT (name#1 = ))\n               +- Filter (isnotnull(customer_id#0) AND NOT (customer_id#0 = ))\n                  +- Relation [customer_id#0,name#1,email#2,phone#3,signup_date#4,extra_info#5,ingestion_timestamp#6,source_file#7,bronze_layer_id#8L] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Join orders with customer information\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m orders_enriched = \u001b[43mjoin_orders_with_customers\u001b[49m\u001b[43m(\u001b[49m\u001b[43morders_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustomers_clean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m log_dataframe_stats(orders_enriched, \u001b[33m\"\u001b[39m\u001b[33morders_enriched\u001b[39m\u001b[33m\"\u001b[39m, logger)\n\u001b[32m      6\u001b[39m display(orders_enriched.limit(\u001b[32m10\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Internviews/fabric-lakehouse/notebooks/../src/silver.py:125\u001b[39m, in \u001b[36mjoin_orders_with_customers\u001b[39m\u001b[34m(orders_df, customers_df)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03mJoin orders with customer data to create enriched dataset.\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    118\u001b[39m \u001b[33;03m    Joined DataFrame with order and customer information\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    120\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mJoining orders with customers...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    122\u001b[39m enriched = (\n\u001b[32m    123\u001b[39m     orders_df\n\u001b[32m    124\u001b[39m     .join(\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m         \u001b[43mcustomers_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcustomer_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43memail\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m    126\u001b[39m         on=\u001b[33m\"\u001b[39m\u001b[33mcustomer_id\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    127\u001b[39m         how=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m     )\n\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# Add flag for orphan orders\u001b[39;00m\n\u001b[32m    130\u001b[39m     .withColumn(\u001b[33m\"\u001b[39m\u001b[33mhas_customer_data\u001b[39m\u001b[33m\"\u001b[39m, F.col(\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m).isNotNull())\n\u001b[32m    131\u001b[39m )\n\u001b[32m    133\u001b[39m total_orders = enriched.count()\n\u001b[32m    134\u001b[39m orphan_orders = enriched.filter(~F.col(\u001b[33m\"\u001b[39m\u001b[33mhas_customer_data\u001b[39m\u001b[33m\"\u001b[39m)).count()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Internviews/fabric-lakehouse/.venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:3223\u001b[39m, in \u001b[36mDataFrame.select\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m   3178\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cols: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mDataFrame\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   3179\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[32m   3180\u001b[39m \n\u001b[32m   3181\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3221\u001b[39m \u001b[33;03m    +-----+---+\u001b[39;00m\n\u001b[32m   3222\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3223\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Internviews/fabric-lakehouse/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Internviews/fabric-lakehouse/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `city` cannot be resolved. Did you mean one of the following? [`email`, `name`, `phone`, `has_email`, `customer_id`].;\n'Project [customer_id#0, name#1, email#271, 'city, 'state]\n+- Project [customer_id#0, name#1, email#271, phone#3, signup_date#4, extra_info#5, ingestion_timestamp#6, source_file#7, bronze_layer_id#8L, has_email#281, (isnotnull(phone#3) AND NOT (phone#3 = )) AS has_phone#292]\n   +- Project [customer_id#0, name#1, email#271, phone#3, signup_date#4, extra_info#5, ingestion_timestamp#6, source_file#7, bronze_layer_id#8L, (isnotnull(email#271) AND NOT (email#271 = )) AS has_email#281]\n      +- Deduplicate [customer_id#0]\n         +- Project [customer_id#0, name#1, lower(trim(email#2, None)) AS email#271, phone#3, signup_date#4, extra_info#5, ingestion_timestamp#6, source_file#7, bronze_layer_id#8L]\n            +- Filter (isnotnull(name#1) AND NOT (name#1 = ))\n               +- Filter (isnotnull(customer_id#0) AND NOT (customer_id#0 = ))\n                  +- Relation [customer_id#0,name#1,email#2,phone#3,signup_date#4,extra_info#5,ingestion_timestamp#6,source_file#7,bronze_layer_id#8L] parquet\n"
     ]
    }
   ],
   "source": [
    "# Join orders with customer information\n",
    "orders_enriched = join_orders_with_customers(orders_clean, customers_clean)\n",
    "\n",
    "log_dataframe_stats(orders_enriched, \"orders_enriched\", logger)\n",
    "\n",
    "display(orders_enriched.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e9ad2",
   "metadata": {},
   "source": [
    "## Step 5: Write Silver Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e183d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write cleaned customers to Silver layer (Parquet format)\n",
    "logger.info(\"Writing customers to Silver layer...\")\n",
    "\n",
    "customers_clean.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{SILVER_PATH}/customers\")\n",
    "\n",
    "logger.info(\"‚úì Customers Silver table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write enriched orders to Silver layer (Parquet format)\n",
    "logger.info(\"Writing orders to Silver layer...\")\n",
    "\n",
    "orders_enriched.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{SILVER_PATH}/orders\")\n",
    "\n",
    "logger.info(\"‚úì Orders Silver table created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8dac88",
   "metadata": {},
   "source": [
    "## üêõ TASK B: Debug the Monthly Revenue Calculation\n",
    "\n",
    "The `compute_monthly_revenue()` function has bugs! \n",
    "\n",
    "**Expected behavior**: Calculate total revenue per month from completed orders.\n",
    "\n",
    "**Known issues**:\n",
    "1. Case-sensitive status filter missing valid orders\n",
    "2. Wrong revenue calculation (not accounting for quantity)\n",
    "\n",
    "**Your task**: \n",
    "1. Run the cell below and observe incorrect results\n",
    "2. Examine the function in `src/silver.py`\n",
    "3. Fix the bugs\n",
    "4. Re-run and verify correct output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c416895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è THIS WILL PRODUCE INCORRECT RESULTS - DEBUG IT!\n",
    "logger.info(\"Computing monthly revenue (BUGGY VERSION)...\")\n",
    "\n",
    "monthly_revenue = compute_monthly_revenue(orders_bronze)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Monthly Revenue (contains bugs):\")\n",
    "display(monthly_revenue)\n",
    "\n",
    "total_revenue = monthly_revenue.select(F.sum(\"revenue\")).collect()[0][0]\n",
    "print(f\"\\nTotal Revenue: ${total_revenue:,.2f}\")\n",
    "print(\"\\n‚ùå This is INCORRECT! The function has bugs that need to be fixed.\")\n",
    "print(\"\\nHints:\")\n",
    "print(\"1. Check how status is being filtered (case sensitivity)\")\n",
    "print(\"2. Verify revenue calculation includes quantity\")\n",
    "print(\"3. Look at the Bronze data - status values have mixed cases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89093ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After fixing the bugs, test with cleaned data\n",
    "# The cleaned orders have normalized status to lowercase\n",
    "logger.info(\"Testing with cleaned Silver data...\")\n",
    "\n",
    "monthly_revenue_clean = compute_monthly_revenue(orders_clean)\n",
    "\n",
    "print(\"\\n‚úì Monthly Revenue (from Silver layer):\")\n",
    "display(monthly_revenue_clean)\n",
    "\n",
    "total_revenue_clean = monthly_revenue_clean.select(F.sum(\"revenue\")).collect()[0][0]\n",
    "print(f\"\\nTotal Revenue: ${total_revenue_clean:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722c8ec8",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87622dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_customers = customers_clean.count()\n",
    "total_orders = orders_enriched.count()\n",
    "\n",
    "print(f\"\"\"\\n{'='*50}\n",
    "Silver Layer Transformation Complete\n",
    "{'='*50}\n",
    "Customers (cleaned): {total_customers}\n",
    "Orders (enriched): {total_orders}\n",
    "\n",
    "Data Quality:\n",
    "  Customer Score: {customer_quality.quality_score:.2f}%\n",
    "  Order Score: {order_quality.quality_score:.2f}%\n",
    "\n",
    "Next Steps:\n",
    "‚Üí Fix bugs in compute_monthly_revenue()\n",
    "‚Üí Run notebook 03_Gold_Aggregates.ipynb for analytics\n",
    "{'='*50}\\n\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
