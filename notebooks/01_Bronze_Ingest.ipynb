{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e03639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:31:16 - __main__ - INFO - Starting Bronze layer ingestion\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.logging_utils import setup_logger, log_dataframe_stats\n",
    "\n",
    "# Setup logger\n",
    "logger = setup_logger(__name__, level=\"INFO\")\n",
    "logger.info(\"Starting Bronze layer ingestion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2010701a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/10 12:31:20 WARN Utils: Your hostname, kdcllc-ThinkPad-P16-Gen-2 resolves to a loopback address: 127.0.1.1; using 192.168.86.233 instead (on interface wlp0s20f3)\n",
      "26/02/10 12:31:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/10 12:31:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:31:21 - __main__ - INFO - Created new Spark session\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session (simplified for local development)\n",
    "try:\n",
    "    spark\n",
    "    logger.info(\"Using existing Spark session\")\n",
    "except NameError:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Bronze_Ingestion\") \\\n",
    "        .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "        .getOrCreate()\n",
    "    logger.info(\"Created new Spark session\")\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"../data\"  # Local development path\n",
    "BRONZE_PATH = \"Tables/bronze\"  # Bronze layer path (uses Parquet format)\n",
    "\n",
    "# Note: Using Parquet format for local development (compatible with Delta in Fabric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03235abc",
   "metadata": {},
   "source": [
    "## Task A: Load Customers Data\n",
    "\n",
    "Read the customers CSV file and create a Bronze Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91eb77c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:31:29 - __main__ - INFO - Reading customers.csv...\n",
      "2026-02-10 12:31:33 - __main__ - INFO - DataFrame 'customers_bronze' statistics:\n",
      "2026-02-10 12:31:33 - __main__ - INFO -   - Rows: 30\n",
      "2026-02-10 12:31:33 - __main__ - INFO -   - Columns: 9\n",
      "2026-02-10 12:31:33 - __main__ - INFO -   - Column names: customer_id, name, email, phone, signup_date, extra_info, ingestion_timestamp, source_file, bronze_layer_id\n",
      "2026-02-10 12:31:33 - __main__ - INFO -   - Null counts:\n",
      "2026-02-10 12:31:33 - __main__ - INFO -     customer_id: 1 (3.33%)\n",
      "2026-02-10 12:31:33 - __main__ - INFO -     name: 1 (3.33%)\n",
      "2026-02-10 12:31:33 - __main__ - INFO -     email: 1 (3.33%)\n",
      "2026-02-10 12:31:33 - __main__ - INFO -     phone: 1 (3.33%)\n",
      "2026-02-10 12:31:33 - __main__ - INFO -     signup_date: 1 (3.33%)\n",
      "2026-02-10 12:31:33 - __main__ - INFO -     extra_info: 24 (80.00%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: string, name: string, email: string, phone: string, signup_date: date, extra_info: string, ingestion_timestamp: timestamp, source_file: string, bronze_layer_id: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read customers CSV\n",
    "logger.info(\"Reading customers.csv...\")\n",
    "\n",
    "customers_raw = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(f\"{DATA_PATH}/customers.csv\")\n",
    "\n",
    "# Add metadata columns for lineage\n",
    "customers_bronze = customers_raw \\\n",
    "    .withColumn(\"ingestion_timestamp\", F.current_timestamp()) \\\n",
    "    .withColumn(\"source_file\", F.lit(\"customers.csv\")) \\\n",
    "    .withColumn(\"bronze_layer_id\", F.monotonically_increasing_id())\n",
    "\n",
    "# Log statistics\n",
    "log_dataframe_stats(customers_bronze, \"customers_bronze\", logger)\n",
    "\n",
    "# Display sample\n",
    "display(customers_bronze.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "762c2034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:31:38 - __main__ - INFO - Writing customers to Bronze table...\n",
      "2026-02-10 12:31:38 - __main__ - INFO - ✓ Customers Bronze table created successfully\n"
     ]
    }
   ],
   "source": [
    "# Write to Parquet (Bronze layer)\n",
    "# Note: In Azure Fabric, this would be Delta format\n",
    "logger.info(\"Writing customers to Bronze table...\")\n",
    "\n",
    "customers_bronze.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{BRONZE_PATH}/customers\")\n",
    "\n",
    "logger.info(\"✓ Customers Bronze table created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f763fd",
   "metadata": {},
   "source": [
    "## Task A: Load Orders Data\n",
    "\n",
    "Read the orders CSV file and create a Bronze Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1051d731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:31:43 - __main__ - INFO - Reading orders.csv...\n",
      "2026-02-10 12:31:43 - __main__ - INFO - DataFrame 'orders_bronze' statistics:\n",
      "2026-02-10 12:31:43 - __main__ - INFO -   - Rows: 36\n",
      "2026-02-10 12:31:43 - __main__ - INFO -   - Columns: 10\n",
      "2026-02-10 12:31:43 - __main__ - INFO -   - Column names: order_id, order_date, customer_id, status, quantity, price, notes, ingestion_timestamp, source_file, bronze_layer_id\n",
      "2026-02-10 12:31:44 - __main__ - INFO -   - Null counts:\n",
      "2026-02-10 12:31:44 - __main__ - INFO -     order_date: 1 (2.78%)\n",
      "2026-02-10 12:31:44 - __main__ - INFO -     customer_id: 1 (2.78%)\n",
      "2026-02-10 12:31:44 - __main__ - INFO -     status: 1 (2.78%)\n",
      "2026-02-10 12:31:44 - __main__ - INFO -     quantity: 1 (2.78%)\n",
      "2026-02-10 12:31:44 - __main__ - INFO -     price: 1 (2.78%)\n",
      "2026-02-10 12:31:44 - __main__ - INFO -     notes: 21 (58.33%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[order_id: int, customer_id: string, order_date: date, status: string, quantity: int, price: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read orders CSV\n",
    "logger.info(\"Reading orders.csv...\")\n",
    "\n",
    "orders_raw = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(f\"{DATA_PATH}/orders.csv\")\n",
    "\n",
    "# Add metadata columns for lineage\n",
    "orders_bronze = orders_raw \\\n",
    "    .withColumn(\"ingestion_timestamp\", F.current_timestamp()) \\\n",
    "    .withColumn(\"source_file\", F.lit(\"orders.csv\")) \\\n",
    "    .withColumn(\"bronze_layer_id\", F.monotonically_increasing_id())\n",
    "\n",
    "# Log statistics\n",
    "log_dataframe_stats(orders_bronze, \"orders_bronze\", logger)\n",
    "\n",
    "# Display sample with various status cases\n",
    "display(orders_bronze.select(\"order_id\", \"customer_id\", \"order_date\", \"status\", \"quantity\", \"price\").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e55c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:31:55 - __main__ - INFO - Writing orders to Bronze table...\n",
      "2026-02-10 12:31:55 - __main__ - INFO - ✓ Orders Bronze table created successfully\n"
     ]
    }
   ],
   "source": [
    "# Write to Parquet (Bronze layer)\n",
    "# Note: In Azure Fabric, this would be Delta format\n",
    "logger.info(\"Writing orders to Bronze table...\")\n",
    "\n",
    "orders_bronze.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{BRONZE_PATH}/orders\")\n",
    "\n",
    "logger.info(\"✓ Orders Bronze table created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22694c5",
   "metadata": {},
   "source": [
    "## Data Quality Summary\n",
    "\n",
    "Quick analysis of data quality issues in the Bronze layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f345d194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:32:00 - __main__ - INFO - Analyzing order status values...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[status: string, count: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:32:00 - __main__ - INFO - ⚠️ Note: Status values have different cases (Complete, COMPLETE, complete)\n",
      "2026-02-10 12:32:00 - __main__ - INFO -    This will be addressed in the Silver layer transformation\n",
      "2026-02-10 12:32:00 - __main__ - INFO -    This will be addressed in the Silver layer transformation\n"
     ]
    }
   ],
   "source": [
    "# Check status value variations (important for Task B)\n",
    "logger.info(\"Analyzing order status values...\")\n",
    "\n",
    "status_distribution = orders_bronze.groupBy(\"status\").count().orderBy(\"status\")\n",
    "display(status_distribution)\n",
    "\n",
    "logger.info(\"⚠️ Note: Status values have different cases (Complete, COMPLETE, complete)\")\n",
    "logger.info(\"   This will be addressed in the Silver layer transformation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d57e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Bronze Layer Ingestion Complete\n",
      "==================================================\n",
      "Customers ingested: 30\n",
      "Orders ingested: 36\n",
      "\n",
      "Next Steps:\n",
      "→ Run notebook 02_Silver_Transform.ipynb to clean and transform data\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "total_customers = customers_bronze.count()\n",
    "total_orders = orders_bronze.count()\n",
    "\n",
    "print(f\"\"\"\\n{'='*50}\n",
    "Bronze Layer Ingestion Complete\n",
    "{'='*50}\n",
    "Customers ingested: {total_customers}\n",
    "Orders ingested: {total_orders}\n",
    "\n",
    "Next Steps:\n",
    "→ Run notebook 02_Silver_Transform.ipynb to clean and transform data\n",
    "{'='*50}\\n\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
